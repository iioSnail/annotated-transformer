{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch入门实战（7）：基于BERT实现简单的中文文本摘要任务（Summarization task）"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iioSnail/chaotic-transformer-tutorials/blob/master/bert_classification_demo.ipynb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 如果你没有使用Google Drive，请不要运行这个代码块\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[0;32m      3\u001B[0m drive\u001B[38;5;241m.\u001B[39mmount(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/drive\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# 如果你没有使用Google Drive，请不要运行这个代码块\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 本文涉及知识点\n",
    "\n",
    "1. [nn.Transformer的使用](https://blog.csdn.net/zhaohongfei_358/article/details/126019181)\n",
    "2. [Transformer源码解读](https://blog.csdn.net/zhaohongfei_358/article/details/126085246) (了解即可)\n",
    "3. [Pytorch中DataLoader和Dataset的基本用法](https://blog.csdn.net/zhaohongfei_358/article/details/122742656)\n",
    "4. [Masked-Attention的机制和原理](https://blog.csdn.net/zhaohongfei_358/article/details/125858248)\n",
    "5. [Pytorch自定义损失函数](https://blog.csdn.net/zhaohongfei_358/article/details/125759911)\n",
    "6. [Hugging Face快速入门](https://blog.csdn.net/zhaohongfei_358/article/details/126224199)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 本文内容\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 环境配置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "本文重点依赖Hugging Face的两个重要类库datasets和transformers，所以需要安装：\n",
    "\n",
    "```\n",
    "transformers==4.21\n",
    "datasets==2.4\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入本文要使用的所有依赖包:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 用于加载bert-base-chinese模型的分词器\n",
    "from transformers import AutoTokenizer\n",
    "# 用于加载bert-base-chinese模型\n",
    "from transformers import AutoModel\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 全局配置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义一些全局变量，我是不太喜欢一些全局变量在函数中传来传去的，太麻烦了。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "# 文本的最大长度\n",
    "text_max_length = 128\n",
    "epochs = 1000\n",
    "validation_ratio = 0.1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 每多少步，打印一次loss\n",
    "log_per_step = 20\n",
    "# 每多少步存储一次模型\n",
    "save_per_step = 5000\n",
    "\n",
    "dataset_dir = Path(\"./dataset\")\n",
    "\n",
    "# 模型存储路径\n",
    "model_dir = Path(\"./drive/MyDrive/model/transformer_checkpoints\")\n",
    "# 如果工作目录不存在，则创建一个\n",
    "os.makedirs(model_dir) if not os.path.exists(model_dir) else ''\n",
    "\n",
    "print(\"Device:\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 加载数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "pd_data = pandas.read_csv(dataset_dir / 'train.csv')[['text', 'target']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "加载成功后，来看一下内容："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text  target\n3031  Put the RIGHT person up on the block #Shelli??...       0\n1204  I'm mentally preparing myself for a bomb ass s...       0\n220   Cop pulls drunk driver to safety SECONDS befor...       1\n4629  Enter the world of extreme diving ÛÓ 9 storie...       0\n7187  BUT I will be uploading these videos ASAP so y...       0\n7202  @Camilla_33 @CrayKain Hate to shatter your del...       0\n2847  Could Billboard's Hot 100 chart be displaced b...       1\n3262  @suelinflower there is no words to describe th...       0\n3675     @Bardissimo Yes life has a 100% fatality rate.       0\n965   @TR_jdavis Bruh you wanna fight I'm down meet ...       0\n5768  @Trollkrattos Juan Carlos Salvador The Secret ...       0\n763                 I blew up snapchat for no reason ??       0\n7193  BREAKING: Obama Officials GAVE Muslim Terroris...       1\n3529  DK Eyewitness Travel Guide: Denmark: travel gu...       0\n1095  @SweetieBirks @mirrorlady2 @SLATUKIP So name a...       1\n3513  #Eyewitness media is actively embraced by #UK ...       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3031</th>\n      <td>Put the RIGHT person up on the block #Shelli??...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1204</th>\n      <td>I'm mentally preparing myself for a bomb ass s...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>Cop pulls drunk driver to safety SECONDS befor...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4629</th>\n      <td>Enter the world of extreme diving ÛÓ 9 storie...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7187</th>\n      <td>BUT I will be uploading these videos ASAP so y...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7202</th>\n      <td>@Camilla_33 @CrayKain Hate to shatter your del...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2847</th>\n      <td>Could Billboard's Hot 100 chart be displaced b...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>@suelinflower there is no words to describe th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3675</th>\n      <td>@Bardissimo Yes life has a 100% fatality rate.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>965</th>\n      <td>@TR_jdavis Bruh you wanna fight I'm down meet ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5768</th>\n      <td>@Trollkrattos Juan Carlos Salvador The Secret ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>I blew up snapchat for no reason ??</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7193</th>\n      <td>BREAKING: Obama Officials GAVE Muslim Terroris...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3529</th>\n      <td>DK Eyewitness Travel Guide: Denmark: travel gu...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1095</th>\n      <td>@SweetieBirks @mirrorlady2 @SLATUKIP So name a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3513</th>\n      <td>#Eyewitness media is actively embraced by #UK ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.sample(16, random_state=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset And Dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "pd_validation_data = pd_data.sample(frac=validation_ratio)\n",
    "pd_train_data = pd_data[~pd_data.index.isin(pd_validation_data.index)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "加载好数据集后，我们就可以开始构建Dataset了，我们这里Dataset就是返回评论和其摘要："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train'):\n",
    "        super(MyDataset, self).__init__()\n",
    "        # 拿到对应的数据\n",
    "        if mode == 'train':\n",
    "            self.dataset = pd_train_data\n",
    "        elif mode == 'validation':\n",
    "            self.dataset = pd_validation_data\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode {}\".format(mode))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 取第index条\n",
    "        data = self.dataset.iloc[index]\n",
    "        # 取其评论\n",
    "        source = data['text'].replace(\"#\", \"\").replace(\"@\", \"\")\n",
    "        # 取对应的摘要\n",
    "        target = data['target']\n",
    "        # 返回\n",
    "        return source, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_dataset = MyDataset('train')\n",
    "validation_dataset = MyDataset('validation')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们来打印看一下；"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n 1)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "构造好Dataset后，就可以来构造Dataloader了。在构造Dataloader前，我们需要先定义好分词器："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们来尝试使用一下分词器："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 1045, 1005, 1049, 4083, 2784, 4083,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I'm learning deep learning\", return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以正常运行。其中101表示“开始”(`[CLS]`)，102表示句子结束(`[SEP]`)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们接着构造我们的Dataloader。我们需要定义一下collate_fn，在其中完成对句子进行编码、填充、组装batch等动作："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将一个batch的文本句子转成tensor，并组成batch。\n",
    "    :param batch: 一个batch的句子，例如: [('评论', '摘要'), ('评论', '摘要'), ...]\n",
    "    :return: 处理后的结果，例如：\n",
    "             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}\n",
    "             tgt和tgt_y与src格式一样\n",
    "             n_tokens为本轮预测时有效token数\n",
    "    \"\"\"\n",
    "    text, target = zip(*batch)\n",
    "    text, target = list(text), list(target)\n",
    "\n",
    "    # src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可\n",
    "    # padding='max_length' 不够长度的进行填充\n",
    "    # truncation=True 长度过长的进行裁剪\n",
    "    src = tokenizer(text, padding='max_length', max_length=text_max_length, return_tensors='pt', truncation=True)\n",
    "\n",
    "    return src, torch.LongTensor(target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们来看一眼train_loader的数据："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(targets)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 构建模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # 加载bert模型\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # 最后的预测层\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        前向传播，获取decoder的输出。注意是decoder的输出，不是最后线性层的输出\n",
    "        :param src: 分词后的评论数据\n",
    "        :param tgt: 前面累计预测出的结果\n",
    "        :return: decoder的输出\n",
    "        \"\"\"\n",
    "        # 将src直接序列解包传入bert，因为bert和tokenizer是一套的，所以可以这么做。\n",
    "        # 得到encoder的输出\n",
    "        outputs = self.bert(**src).last_hidden_state[:, 0, :]\n",
    "        return self.predictor(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4655],\n        [0.4628],\n        [0.4697],\n        [0.4832],\n        [0.4707],\n        [0.4490],\n        [0.4764],\n        [0.4553],\n        [0.4642],\n        [0.4575],\n        [0.4581],\n        [0.4649],\n        [0.4669],\n        [0.4916],\n        [0.4559],\n        [0.4711]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs.to(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来开始正式训练模型，首先定义出损失函数和优化器："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "criteria = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 由于src，tgt都是字典类型的，定义一个辅助函数帮助to(device)\n",
    "def to_device(dict_tensors):\n",
    "    result_tensors = {}\n",
    "    for key, value in dict_tensors.items():\n",
    "        result_tensors[key] = value.to(device)\n",
    "    return result_tensors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def validate():\n",
    "    total_loss = 0.\n",
    "    total_correct = 0\n",
    "    for inputs, targets in validation_loader:\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        correct_num = (((outputs >= 0.5).float() * 1).flatten() == targets).sum()\n",
    "        total_correct += correct_num\n",
    "\n",
    "    return total_correct / len(validation_dataset), total_loss / len(validation_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "开始训练："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# 首先将模型调成训练模式\n",
    "model.train()\n",
    "\n",
    "# 清空一下cuda缓存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 定义几个变量，帮助打印loss\n",
    "total_loss = 0.\n",
    "# 记录步数\n",
    "step = 0\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # 从batch中拿到训练数据\n",
    "        inputs, targets = to_device(inputs), targets.to(device)\n",
    "        # 传入模型进行前向传递\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criteria(outputs.view(-1), targets.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        step += 1\n",
    "\n",
    "        if step % log_per_step == 0:\n",
    "            print(\"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}\".format(epoch+1, epochs, i, len(train_loader), total_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "        del inputs, targets\n",
    "\n",
    "    accuracy, validation_loss = validate()\n",
    "    print(\"Epoch {}, accuracy: {:.4f}, validation loss: {:.4f}\".format(epoch+1, accuracy, validation_loss))\n",
    "    torch.save(model, model_dir / f\"model_{epoch}.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型使用"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}