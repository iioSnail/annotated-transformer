{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch入门实战（7）：基于BERT实现简单的中文文本摘要任务（Summarization task）"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Globe Config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "text_max_length = 512\n",
    "summary_max_length = 48\n",
    "epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset amazon_reviews_multi (C:\\Users\\zhaohongfei1\\.cache\\huggingface\\datasets\\amazon_reviews_multi\\zh\\1.0.0\\724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b34f2ebaeff5459c8638c7d81f400082"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"amazon_reviews_multi\", \"zh\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'review_id': 'zh_0626061',\n 'product_id': 'product_zh_0691762',\n 'reviewer_id': 'reviewer_zh_0824776',\n 'stars': 1,\n 'review_body': '本人账号被盗，资金被江西（杨建）挪用，请亚马逊尽快查实，将本人的200元资金退回。本人已于2017年11月30日提交退货申请，为何到2018年了还是没解决？亚马逊是什么情况？请给本人一个合理解释。',\n 'review_title': '此书不是本人购买',\n 'language': 'zh',\n 'product_category': 'book'}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset And Dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train'):\n",
    "        super(SummarizationDataset, self).__init__()\n",
    "        self.dataset = dataset[mode]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        text = data['review_body']\n",
    "        summary = data['review_title']\n",
    "        return text, summary\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.dataset)\n",
    "        return 8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_dataset = SummarizationDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # print(batch)\n",
    "    text, summary = zip(*batch)\n",
    "    text, summary = list(text), list(summary)\n",
    "\n",
    "    # src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可\n",
    "    src = tokenizer(text, padding='max_length', max_length=text_max_length, return_tensors='pt', truncation=True)\n",
    "    tgt = tokenizer(summary, padding='max_length', max_length=summary_max_length, return_tensors='pt', truncation=True)\n",
    "\n",
    "    tgt_y = {}\n",
    "    for key, value in tgt.items():\n",
    "        tgt_y[key] = value[:, 1:]\n",
    "\n",
    "    for key, value in tgt.items():\n",
    "        tgt[key] = value[:, :-1]\n",
    "\n",
    "    n_tokens = tgt_y['attention_mask'].sum().item()\n",
    "\n",
    "    return src, tgt, tgt_y, n_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class SummarizationModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SummarizationModel, self).__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=768, nhead=8, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "        self.embeddings = self.bert.embeddings\n",
    "        self.predictor = nn.Linear(768, tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        last_hidden_state = self.bert(**src).last_hidden_state\n",
    "        decoder_inputs = self.embeddings(tgt['input_ids'])\n",
    "        decoder_outputs = self.decoder(tgt=decoder_inputs, memory=last_hidden_state)\n",
    "        return decoder_outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SummarizationModel()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class SummarizationLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SummarizationLoss, self).__init__()\n",
    "        self.criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, outputs, tgt_y, n_tokens):\n",
    "        targets = tgt_y['input_ids'].flatten()\n",
    "        outputs = outputs.view(-1, tokenizer.vocab_size)\n",
    "        outputs[targets == 0] = 0\n",
    "        return self.criteria(outputs, targets) / n_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "criteria = SummarizationLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss:{} tensor(0.2428, grad_fn=<AddBackward0>)\n",
      "total loss:{} tensor(1.4079, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "total_loss = 0.\n",
    "step = 0\n",
    "\n",
    "def to_device(dict_tensors):\n",
    "    result_tensors = {}\n",
    "    for key, value in dict_tensors.items():\n",
    "        result_tensors[key] = value.to(device)\n",
    "    return result_tensors\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        src, tgt, tgt_y, n_tokens = batch\n",
    "        src, tgt, tgt_y = to_device(src), to_device(tgt), to_device(tgt_y)\n",
    "        outputs = model(src, tgt)\n",
    "        outputs = model.predictor(outputs)\n",
    "        loss = criteria(outputs, tgt_y, n_tokens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss\n",
    "        step += 1\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            print(\"total loss:{}\", total_loss)\n",
    "            total_loss = 0\n",
    "\n",
    "        del src, tgt, tgt_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}