{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HuggingFace简介"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Hugging Face Hub](https://huggingface.co/docs/hub/index)和 Github 类似，都是Hub(社区)。Hugging Face可以说的上是机器学习界的Github。Hugging Face为用户提供了以下主要功能：\n",
    "\n",
    "- [模型仓库（Model Repository）](https://huggingface.co/docs/hub/repositories-getting-started)：Git仓库可以让你管理代码版本、开源代码。而模型仓库可以让你管理模型版本、开源模型等。使用方式与Github类似。\n",
    "- [模型（Models）](https://huggingface.co/docs/hub/models)：Hugging Face为不同的机器学习任务提供了许多[预训练好的机器学习模型](https://huggingface.co/models)供大家使用，这些模型就存储在模型仓库中。\n",
    "- [数据集（Dataset）](https://huggingface.co/docs/hub/datasets)：Hugging Face上有许多公开数据集。\n",
    "\n",
    "hugging face在NLP领域最出名，其提供的模型大多都是基于Transformer的。为了易用性，Hugging Face还为用户提供了以下几个项目：\n",
    "\n",
    "  - **Transformers**([github](https://github.com/huggingface/transformers), [官方文档](https://huggingface.co/docs/transformers/index)): Transformers提供了上千个预训练好的模型可以用于不同的任务，例如文本领域、音频领域和CV领域。该项目是HuggingFace的核心，可以说学习HuggingFace就是在学习该项目如何使用。\n",
    "  - **Datasets**([github](https://github.com/huggingface/datasets), [官方文档](https://huggingface.co/docs/datasets/index)): 一个轻量级的数据集框架，主要有两个功能：①一行代码下载和预处理常用的公开数据集； ② 快速、易用的数据预处理类库。\n",
    "  - **Accelerate**([github](https://github.com/huggingface/accelerate), [官方文档](https://huggingface.co/docs/accelerate/index)): 帮助Pytorch用户很方便的实现 multi-GPU/TPU/fp16。\n",
    "  - **Space**([链接](https://huggingface.co/spaces))：Space提供了许多好玩的深度学习应用，可以尝试玩一下。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transforms简介\n",
    "\n",
    "Hugging Face Transformer是Hugging Face最核心的项目，你可以用它做以下事情：\n",
    "\n",
    "- 直接使用预训练模型进行推理\n",
    "- 提供了大量预训练模型可供使用\n",
    "- 使用预训练模型进行迁移学习"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers安装\n",
    "\n",
    "安装Transformers非常简单，直接安装即可。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 使用Transformers进行推理\n",
    "\n",
    "如果你的任务是一个比较常见的，大概率可以直接使用Transformer提供的[Pipeline](https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/pipelines)API解决，其使用方式非常简单，可以说是直接用即可。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "D:\\Anaconda3\\envs\\transformer\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': ' quel âge êtes-vous?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "print(translator(\"How old are you?\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对于部分特定任务，官方并没有提供相应的模型，但你也可以到[官网搜索模型](https://huggingface.co/models)，然后显示指定即可。在加载模型时，你有可能会因为缺少一些库而报错，这个时候，只需要安装对应的库，然后重启即可。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'translation_text': '我在学习深思熟虑'}]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_zh\", model='Helsinki-NLP/opus-mt-en-zh')\n",
    "translator(\"I'm learning deep learning.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "更多Pipeline请参考[官方文档](https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/pipelines)：https://huggingface.co/docs/transformers/v4.21.0/en/main_classes/pipelines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 查找Hugging Face模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "本节来介绍一下如何通过Hugging Face找到你需要的模型。\n",
    "\n",
    "首先，我们需要到来到官网的[模型模块](https://huggingface.co/models)。之后我们会看到如下界面：\n",
    "\n",
    "<img src=\"./images/hf_0.jpg\" width=\"800\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "其主要包含三部分：\n",
    "\n",
    "1. **Filter**: 用于筛选你想要的模型\n",
    "2. **模型列表**: 展示了可使用的模型。不带前缀的是官方提供的模型，例如gpt2，而带前缀的是第三方提供的模型。\n",
    "3. **搜索框**：你可以通过搜索框按名字搜索模型。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当你点进去你的模型后，你会来到如下页面：\n",
    "\n",
    "<img src=\"./images/hf_1.png\" width=1000>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "该页面主要的几个部分：\n",
    "\n",
    "1. **模型介绍（Model Card）**: 我们可以通过该文档查看该模型都提供了哪些功能，模型的表现等。\n",
    "2. **模型文件（Files and versions)**: 从该模块可以下载模型文件，一般包含多种框架的（TF、Pytorch等）模型文件和配置文件等，可以用于离线加载。\n",
    "3. **测试模型(Hosted inference API)**: 可以直接通过该模块测试自己的模型。同时Hugging Face也提供了Http API可以调用，这样就不需要本地部署了。详情请参考：https://huggingface.co/docs/api-inference/index\n",
    "4. **使用该模型的应用（Spaces using ..）**：这里展示了使用该模型的应用，可以点进去玩一玩。\n",
    "5. **代码样例（Use in Transformers）**：你可以通过该模块直接查看该模型的使用方式，直接拷贝代码到项目里就可以用了。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 使用Hugging Face模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transformers项目提供了几个简单的API帮助用户使用Hugging Face模型，而这几个简单的API统称为`AutoClass`([官方文档链接](https://huggingface.co/docs/transformers/autoclass_tutorial))，包括：\n",
    "\n",
    "1. `AutoTokenizer`: 用于文本分词\n",
    "2. `AutoFeatureExtractor`: 用于特征提取\n",
    "3. `AutoProcessor`: 用于数据处理\n",
    "4. `AutoModel`: 用于加载模型\n",
    "\n",
    "它们的使用方式均为: `AutoClass.from_pretrain(\"模型名称\")`，然后就可以用了。例如："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer(\"I'm learning deep learning.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 1045, 1005, 1049, 4083, 2784, 4083, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "通常一个模型会包含上述4个中的部分功能，例如，对于`bert-base-uncased`模型，就包含“分词”和“模型”两项功能，我们可以通过**代码样例（Use in Transformers）** 模块查看：\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/hf_2.png\" width=\"600\">\n",
    "\n",
    "> 也不是所有的模型都可以使用`AutoModel`，具体还要看模型的代码示例。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 迁移学习"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "很多情况下，Hugging Face提供的模型并不能满足我们的需要，所以我们还是要自己训练模型的。此时我们可以使用Hugging Face提供的预训练模型来进行迁移学习，本节将会介绍如何使用Hugging Face进行迁移学习。\n",
    "\n",
    "使用Hugging Face模型做迁移学习的思路和普通迁移学习几乎一致：\n",
    "\n",
    "1. 首先选择一个和你的任务类似的任务的预训练模型，或者直接选择一个任务无关的基础模型。\n",
    "2. 从原有模型中拿出主干部分(backbone)\n",
    "3. 然后接上自己的下游任务，构建成新的模型\n",
    "4. 开始训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里我以`bert-base-uncased`模型作为例子，进行一次模型参数更新操作，假设我的任务是一个二分类的情感分类问题。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "首先，我们将该模型的**Use in Transformers**中的样例代码拷贝过来："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "之后我们需要尝试使用一下该模型："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 4083, 2003, 1037, 2200, 3407,  103, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Learning is a very happy [MASK].\", return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1012, 1012, 2003, 1037, 1037, 3407, 2518, 1012, 1012]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer(\"Learning is a very happy thing.\", return_tensors='pt')).logits.argmax(dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "['SPECIAL_TOKENS_ATTRIBUTES',\n '__annotations__',\n '__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__len__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_add_tokens',\n '_additional_special_tokens',\n '_auto_class',\n '_batch_encode_plus',\n '_bos_token',\n '_cls_token',\n '_convert_encoding',\n '_convert_id_to_token',\n '_convert_token_to_id_with_added_voc',\n '_create_or_get_repo',\n '_decode',\n '_decode_use_source_tokenizer',\n '_encode_plus',\n '_eos_token',\n '_eventual_warn_about_too_long_sequence',\n '_eventually_correct_t5_max_length',\n '_from_pretrained',\n '_get_padding_truncation_strategies',\n '_get_repo_url_from_name',\n '_mask_token',\n '_pad',\n '_pad_token',\n '_pad_token_type_id',\n '_processor_class',\n '_push_to_hub',\n '_save_pretrained',\n '_sep_token',\n '_set_processor_class',\n '_tokenizer',\n '_unk_token',\n 'add_special_tokens',\n 'add_tokens',\n 'additional_special_tokens',\n 'additional_special_tokens_ids',\n 'all_special_ids',\n 'all_special_tokens',\n 'all_special_tokens_extended',\n 'as_target_tokenizer',\n 'backend_tokenizer',\n 'batch_decode',\n 'batch_encode_plus',\n 'bos_token',\n 'bos_token_id',\n 'build_inputs_with_special_tokens',\n 'can_save_slow_tokenizer',\n 'clean_up_tokenization',\n 'cls_token',\n 'cls_token_id',\n 'convert_ids_to_tokens',\n 'convert_tokens_to_ids',\n 'convert_tokens_to_string',\n 'create_token_type_ids_from_sequences',\n 'decode',\n 'decoder',\n 'deprecation_warnings',\n 'do_lower_case',\n 'encode',\n 'encode_plus',\n 'eos_token',\n 'eos_token_id',\n 'from_pretrained',\n 'get_added_vocab',\n 'get_special_tokens_mask',\n 'get_vocab',\n 'init_inputs',\n 'init_kwargs',\n 'is_fast',\n 'mask_token',\n 'mask_token_id',\n 'max_len_sentences_pair',\n 'max_len_single_sentence',\n 'max_model_input_sizes',\n 'model_input_names',\n 'model_max_length',\n 'name_or_path',\n 'num_special_tokens_to_add',\n 'pad',\n 'pad_token',\n 'pad_token_id',\n 'pad_token_type_id',\n 'padding_side',\n 'prepare_for_model',\n 'prepare_seq2seq_batch',\n 'pretrained_init_configuration',\n 'pretrained_vocab_files_map',\n 'push_to_hub',\n 'register_for_auto_class',\n 'sanitize_special_tokens',\n 'save_pretrained',\n 'save_vocabulary',\n 'sep_token',\n 'sep_token_id',\n 'set_truncation_and_padding',\n 'slow_tokenizer_class',\n 'special_tokens_map',\n 'special_tokens_map_extended',\n 'tokenize',\n 'train_new_from_iterator',\n 'truncate_sequences',\n 'truncation_side',\n 'unk_token',\n 'unk_token_id',\n 'verbose',\n 'vocab',\n 'vocab_files_names',\n 'vocab_size']"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}